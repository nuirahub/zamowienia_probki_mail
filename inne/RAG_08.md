1. Konfiguracja PoÅ‚Ä…czenia
Najpierw musimy ustawiÄ‡ poÅ‚Ä…czenie, z ktÃ³rego skorzystajÄ… oba narzÄ™dzia.

Python

import psycopg2
from langchain_community.utilities import SQLDatabase
from langchain_postgres import PGVector
from langchain_community.embeddings import OllamaEmbeddings # Lub OpenAIEmbeddings

# Konfiguracja poÅ‚Ä…czenia do bazy
DB_URI = "postgresql+psycopg2://user:password@localhost:5432/twoja_baza"

# 1. Obiekt do SQL Tool (LangChain wrapper na SQLAlchemy)
db = SQLDatabase.from_uri(DB_URI)

# 2. Obiekt do Vector Store
# WaÅ¼ne: collection_name to nazwa tabeli, ktÃ³rÄ… stworzyÅ‚aÅ› w SQL (meeting_notes_embeddings)
embeddings = OllamaEmbeddings(model="nomic-embed-text") # PrzykÅ‚ad dla lokalnego modelu
vector_store = PGVector(
    embeddings=embeddings,
    collection_name="meeting_notes_embeddings",
    connection=DB_URI,
    use_jsonb=True,
)
2. Implementacja run_vector_search (Notatki + ID Lookup)
Tutaj realizujemy strategiÄ™: Najpierw znajdÅº ID klienta, potem filtruj notatki.

Python

def get_client_id_by_name(name: str) -> int | None:
    """
    Pomocnicza funkcja, ktÃ³ra zamienia "BudPol" na 104.
    UÅ¼ywamy czystego SQL dla szybkoÅ›ci.
    """
    # UwaÅ¼aj na SQL Injection w produkcji! Tutaj uproszczony przykÅ‚ad.
    clean_name = name.replace("'", "") 
    query = f"SELECT id FROM clients WHERE name ILIKE '%{clean_name}%' LIMIT 1;"
    
    try:
        # UÅ¼ywamy db z LangChain do wykonania surowego zapytania
        result_str = db.run(query) 
        # db.run zwraca string, np. "[(104,)]" lub pusty, trzeba to sparsowaÄ‡
        import ast
        result = ast.literal_eval(result_str)
        if result and len(result) > 0:
            return result[0][0] # Zwraca ID, np. 104
    except Exception as e:
        print(f"BÅ‚Ä…d przy szukaniu klienta: {e}")
    return None

def run_vector_tool(search_query: str, client_name: str = None, time_frame: str = None):
    """
    GÅ‚Ã³wna funkcja wykonawcza dla narzÄ™dzia vector_search.
    """
    filter_args = {}

    # KROK 1: RozwiÄ…zywanie nazwy klienta na ID
    if client_name:
        client_id = get_client_id_by_name(client_name)
        if client_id:
            # SkÅ‚adnia filtra dla PGVector (zaleÅ¼y od wersji, zazwyczaj dict mapuje na metadane lub kolumny)
            # W langchain-postgres filter dziaÅ‚a na metadanych JSONB lub kolumnach zewnÄ™trznych
            # Tutaj zakÅ‚adamy filtr na kolumnÄ™ relacyjnÄ… 'client_id' jeÅ›li vector store to wspiera,
            # lub czÄ™Å›ciej: filtrujemy po metadanych (musisz dodawaÄ‡ client_id do metadanych przy zapisie!)
            
            filter_args["client_id"] = client_id 
            print(f"ğŸ” DEBUG: Znaleziono ID klienta: {client_id}. FiltrujÄ™ wyniki.")
        else:
            print(f"âš ï¸ DEBUG: Nie znaleziono klienta o nazwie '{client_name}'. Szukam w caÅ‚ej bazie.")

    # KROK 2: Wyszukiwanie (Similarity Search)
    # k=5 oznacza pobranie 5 najbardziej pasujÄ…cych fragmentÃ³w
    docs = vector_store.similarity_search(
        search_query,
        k=5,
        filter=filter_args if filter_args else None
    )

    # KROK 3: Formatowanie wynikÃ³w dla LLM
    # Nie zwracamy surowych obiektÃ³w Document, tylko czysty tekst
    if not docs:
        return "Nie znaleziono Å¼adnych notatek pasujÄ…cych do zapytania."

    formatted_results = "\n\n".join([
        f"--- Notatka (Data: {doc.metadata.get('date', 'Brak daty')}) ---\n{doc.page_content}" 
        for doc in docs
    ])
    
    return formatted_results
WaÅ¼na uwaga o filtrach w PGVector: Aby filter={"client_id": 104} zadziaÅ‚aÅ‚, przy dodawaniu dokumentÃ³w do bazy (indeksowaniu), musisz upewniÄ‡ siÄ™, Å¼e client_id znajduje siÄ™ w metadanych dokumentu, np.: doc = Document(page_content="...", metadata={"client_id": 104, "date": "..."})

3. Implementacja run_sql_tool (Text-to-SQL)
W przypadku Qwen-8B nie moÅ¼emy polegaÄ‡ na tym, Å¼e model sam idealnie wymyÅ›li zapytanie. UÅ¼yjemy Å‚aÅ„cucha create_sql_query_chain, ktÃ³ry automatycznie wstrzykuje schemat tabel do promptu.

Python

from langchain.chains import create_sql_query_chain
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from langchain_core.prompts import PromptTemplate
from langchain_community.llms import Ollama

# Model do SQL (moÅ¼e mieÄ‡ wyÅ¼sze temperature niÅ¼ Router, np. 0.1)
llm_sql = Ollama(model="qwen2.5:7b", temperature=0.1)

# NarzÄ™dzie wykonawcze (to ono faktycznie puszcza query do bazy)
execute_query = QuerySQLDataBaseTool(db=db)

# ÅaÅ„cuch generowania SQL
write_query = create_sql_query_chain(llm_sql, db)

def run_sql_tool(user_question: str):
    """
    Funkcja generujÄ…ca i wykonujÄ…ca SQL na podstawie pytania naturalnego.
    """
    try:
        # KROK 1: Generowanie SQL
        # create_sql_query_chain sam pobiera schemat tabeli i wstawia go do promptu
        generated_sql = write_query.invoke({"question": user_question})
        
        # Oczyszczanie SQL (Qwen czasem dodaje "Here is the SQL: ```sql ... ```")
        cleaned_sql = generated_sql.replace("```sql", "").replace("```", "").strip()
        # Czasami model zwraca "SQLQuery: SELECT...", trzeba to uciÄ…Ä‡
        if "SQLQuery:" in cleaned_sql:
            cleaned_sql = cleaned_sql.split("SQLQuery:")[1].strip()

        print(f"ğŸ“ DEBUG: Wygenerowany SQL: {cleaned_sql}")

        # KROK 2: Wykonanie SQL
        result = execute_query.invoke(cleaned_sql)
        
        return f"Wynik z bazy danych:\n{result}"

    except Exception as e:
        return f"BÅ‚Ä…d podczas pobierania danych z bazy SQL: {str(e)}"
4. GÅ‚Ã³wna pÄ™tla sterujÄ…ca (Orchestrator)
Teraz spinamy wszystko razem. To jest kod, ktÃ³ry uruchamiasz w swojej aplikacji (np. Streamlit lub API).

Python

# ZakÅ‚adam, Å¼e masz juÅ¼ zdefiniowany `chain` routera z poprzedniej odpowiedzi

def process_user_query(user_input: str):
    # 1. Uruchom Router
    router_response = chain.invoke({"question": user_input})
    
    print(f"ğŸ¤– ROUTER: Wybrano narzÄ™dzie: {router_response.tool_name}")
    print(f"ğŸ§  MYÅšLENIE: {router_response.thinking}")

    final_context = ""

    # 2. Wykonaj odpowiednie narzÄ™dzie
    if router_response.tool_name == "sql_db":
        # JeÅ›li router wybraÅ‚ SQL, bierzemy parametry z sql_input
        # Ale w praktyce SQL tool potrzebuje po prostu oryginalnego pytania, 
        # bo sam sobie z niego wyciÄ…gnie co trzeba.
        params = router_response.sql_input
        if params:
            final_context = run_sql_tool(params.question)
        else:
            # Fallback jeÅ›li model nie wypeÅ‚niÅ‚ inputu
            final_context = run_sql_tool(user_input)

    elif router_response.tool_name == "vector_search":
        params = router_response.vector_input
        if params:
            final_context = run_vector_tool(
                search_query=params.search_query,
                client_name=params.client_name,
                time_frame=params.time_frame
            )
        else:
            final_context = "BÅ‚Ä…d: Router wybraÅ‚ vector_search, ale nie podaÅ‚ parametrÃ³w."

    elif router_response.tool_name == "no_tool":
        return "Nie znalazÅ‚em w bazie wiedzy informacji na ten temat. Czy moÅ¼esz doprecyzowaÄ‡?"

    # 3. Synteza ostatecznej odpowiedzi (Final Generation)
    # Teraz bierzemy "brudne" dane (context) i prosimy LLM o Å‚adnÄ… odpowiedÅº dla czÅ‚owieka.
    
    final_prompt = f"""
    JesteÅ› asystentem sprzedaÅ¼y. Odpowiedz na pytanie uÅ¼ytkownika, korzystajÄ…c z poniÅ¼szych informacji pobranych z systemu.
    
    PYTANIE UÅ»YTKOWNIKA: {user_input}
    
    INFORMACJE Z SYSTEMU (Kontekst):
    {final_context}
    
    JeÅ¼eli informacje z systemu zawierajÄ… bÅ‚Ä…d lub sÄ… puste, powiedz o tym uczciwie.
    Odpowiedz zwiÄ™Åºle i profesjonalnie po polsku.
    """
    
    # UÅ¼ywamy tego samego modelu co do routera lub innego
    final_answer = llm.invoke(final_prompt)
    return final_answer
Podsumowanie techniczne:
Dla SQL: CaÅ‚a trudnoÅ›Ä‡ leÅ¼y w dobrym wygenerowaniu zapytania. create_sql_query_chain robi wiÄ™kszoÅ›Ä‡ roboty, ale dla modelu 8B upewnij siÄ™, Å¼e nazwy kolumn w bazie sÄ… po angielsku i sÄ… "samo-wyjaÅ›niajÄ…ce" (np. total_price zamiast col_a), albo uÅ¼yj parametru db.get_table_info() w prompcie, aby model widziaÅ‚ opisy.

Dla Vector: Kluczem jest funkcja get_client_id_by_name. Bez niej system wektorowy bÄ™dzie "Å›lepy" na to, czyj to jest kontekst.